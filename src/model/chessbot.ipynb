{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import chess\n",
    "import chess.pgn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\AppData\\Local\\Temp\\ipykernel_59240\\736078126.py:1: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class ChessGame(Base):\n",
    "    __tablename__ = 'games'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    pgn = Column(Text)\n",
    "\n",
    "engine = create_engine('sqlite:///../chess_games.db')\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot's next move: g6\n"
     ]
    }
   ],
   "source": [
    "def load_openings_from_pgn(pgn_file):\n",
    "    openings = []\n",
    "    with open(pgn_file) as f:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(f)\n",
    "            if game is None:\n",
    "                break\n",
    "\n",
    "            board = game.board()\n",
    "            moves = []\n",
    "            for move in game.mainline_moves():\n",
    "                moves.append(board.san(move))\n",
    "                board.push(move)\n",
    "\n",
    "            openings.append(moves)\n",
    "    return openings\n",
    "\n",
    "#Function to find matching openings based on played moves\n",
    "def find_matching_openings(played_moves, openings):\n",
    "    \"\"\"\n",
    "    Finding opening works by checking if the played moves match the start of any opening in the opening book.\n",
    "    This is because a chosen opening can be diverged from at any point by the opponent.\n",
    "    This makes the bot more dynamic in the opening phase.\n",
    "    \"\"\"\n",
    "    matching_openings = []\n",
    "    for opening in openings:\n",
    "        if played_moves == opening[:len(played_moves)]:\n",
    "            matching_openings.append(opening)\n",
    "    return matching_openings\n",
    "\n",
    "#Function to choose the next move from matching openings\n",
    "def select_next_move(played_moves, matching_openings):\n",
    "    if not matching_openings:\n",
    "        return None  # No matching opening found, time for engine\n",
    "\n",
    "    # Check if there is a next move available in the matching opening\n",
    "    for opening in matching_openings:\n",
    "        if len(opening) > len(played_moves):\n",
    "            next_move = opening[len(played_moves)]\n",
    "            return next_move\n",
    "    \n",
    "    return None  # No more moves in the opening book, time for engine\n",
    "\n",
    "#Load the openings\n",
    "openings = load_openings_from_pgn(\"eco.pgn\")\n",
    "\n",
    "#Example usage\n",
    "played_moves = ['e4']  \n",
    "matching_openings = find_matching_openings(played_moves, openings)\n",
    "next_move = select_next_move(played_moves, matching_openings)\n",
    "\n",
    "if next_move:\n",
    "    print(f\"Bot's next move: {next_move}\")\n",
    "else:\n",
    "    print(\"No matching opening found, calculate the move using engine logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        #Positional encoding for the transformer\n",
    "        self.positional_encoding = PositionalEncoding(d_model=128)\n",
    "\n",
    "        #Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=2)\n",
    "\n",
    "        #Fully connected layer\n",
    "        self.fc1 = nn.Linear(8*8*128, 4096)  #4096 possible moves\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  \n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  \n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  \n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(-1, 128, 8*8)  #[batch_size, d_model, sequence_length]\n",
    "        x = x.permute(2, 0, 1)  #[sequence_length, batch_size, d_model]\n",
    "\n",
    "        # Positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Transformer encoder\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2).contiguous()  #[batch_size, sequence_length, d_model]\n",
    "        x = x.view(-1, 8*8*128)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "#Positional encoding for the transformer in order to give the model information about the position of the pieces\n",
    "#Uses the sine and cosine functions to encode the position of the board in a unique way\n",
    "#Experimental, might be overkill. Saw somewhere it could be useful for the transformer, but not sure if it is properly implemented here\n",
    "class PositionalEncoding(nn.Module): \n",
    "    def __init__(self, d_model, max_len=64):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        pos = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        _2i = torch.arange(0, d_model, 2).float()\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding = self.encoding.unsqueeze(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:x.size(0), :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\lukas\\AppData\\Local\\Temp\\ipykernel_38924\\2781518139.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('savedModels/cnn_transformer_model_current.pth')) #Load the model from the previous training session\n",
      "Processing Batch 1:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 2:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 3:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 4:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 5:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 6:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 7:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 8:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 9:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 10:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 11:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 12:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 13:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 14:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 15:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 16:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 17:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 18:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 19:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 20:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 21:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 22:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 23:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 24:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 25:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 26:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 27:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 28:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 29:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 30:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 31:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 32:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 33:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 34:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 35:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 36:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 37:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 38:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 39:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 40:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 41:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 42:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 43:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 44:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 45:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 46:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 47:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 48:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 49:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 50:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 51:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 52:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 53:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 54:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 55:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 56:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 57:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 58:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 59:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 60:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 61:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 62:   0%|          | 0/64 [00:00<?, ?it/s]\n",
      "Processing Batch 63: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.76, Accuracy=0.277]\n",
      "Processing Batch 64: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.73, Accuracy=0.282]\n",
      "Processing Batch 65: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.75, Accuracy=0.28] \n",
      "Processing Batch 66: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.75, Accuracy=0.277]\n",
      "Processing Batch 67: 100%|██████████| 64/64 [00:36<00:00,  1.75it/s, Loss=2.73, Accuracy=0.282]\n",
      "Processing Batch 68: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.73, Accuracy=0.282]\n",
      "Processing Batch 69: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.71, Accuracy=0.286]\n",
      "Processing Batch 70: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.72, Accuracy=0.284]\n",
      "Processing Batch 71: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.7, Accuracy=0.284] \n",
      "Processing Batch 72: 100%|██████████| 64/64 [00:39<00:00,  1.63it/s, Loss=2.69, Accuracy=0.287]\n",
      "Processing Batch 73: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.71, Accuracy=0.284]\n",
      "Processing Batch 74: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.71, Accuracy=0.283]\n",
      "Processing Batch 75: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.71, Accuracy=0.284]\n",
      "Processing Batch 76: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.7, Accuracy=0.287] \n",
      "Processing Batch 77: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.7, Accuracy=0.287] \n",
      "Processing Batch 78: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.71, Accuracy=0.283]\n",
      "Processing Batch 79: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.69, Accuracy=0.288]\n",
      "Processing Batch 80: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.7, Accuracy=0.286] \n",
      "Processing Batch 81: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.69, Accuracy=0.287]\n",
      "Processing Batch 82: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.68, Accuracy=0.288]\n",
      "Processing Batch 83: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.7, Accuracy=0.285] \n",
      "Processing Batch 84: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.68, Accuracy=0.289]\n",
      "Processing Batch 85: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.68, Accuracy=0.288]\n",
      "Processing Batch 86: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.68, Accuracy=0.288]\n",
      "Processing Batch 87: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.66, Accuracy=0.291]\n",
      "Processing Batch 88: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.66, Accuracy=0.292]\n",
      "Processing Batch 89: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.68, Accuracy=0.288]\n",
      "Processing Batch 90: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.68, Accuracy=0.289]\n",
      "Processing Batch 91: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.69, Accuracy=0.287]\n",
      "Processing Batch 92: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.66, Accuracy=0.29] \n",
      "Processing Batch 93: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.65, Accuracy=0.291]\n",
      "Processing Batch 94: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.63, Accuracy=0.297]\n",
      "Processing Batch 95: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.64, Accuracy=0.296]\n",
      "Processing Batch 96: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.64, Accuracy=0.296]\n",
      "Processing Batch 97: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.65, Accuracy=0.294]\n",
      "Processing Batch 98: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.65, Accuracy=0.294]\n",
      "Processing Batch 99: 100%|██████████| 64/64 [00:33<00:00,  1.88it/s, Loss=2.66, Accuracy=0.292]\n",
      "Processing Batch 100: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.65, Accuracy=0.295]\n",
      "Processing Batch 101: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.65, Accuracy=0.294]\n",
      "Processing Batch 102: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.67, Accuracy=0.291]\n",
      "Processing Batch 103: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.67, Accuracy=0.288]\n",
      "Processing Batch 104: 100%|██████████| 64/64 [00:33<00:00,  1.88it/s, Loss=2.64, Accuracy=0.293]\n",
      "Processing Batch 105: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.65, Accuracy=0.292]\n",
      "Processing Batch 106: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.63, Accuracy=0.295]\n",
      "Processing Batch 107: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.63, Accuracy=0.294]\n",
      "Processing Batch 108: 100%|██████████| 64/64 [00:36<00:00,  1.77it/s, Loss=2.63, Accuracy=0.298]\n",
      "Processing Batch 109: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.64, Accuracy=0.295]\n",
      "Processing Batch 110: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.61, Accuracy=0.301]\n",
      "Processing Batch 111: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.63, Accuracy=0.292]\n",
      "Processing Batch 112: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.62, Accuracy=0.295]\n",
      "Processing Batch 113: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.61, Accuracy=0.298]\n",
      "Processing Batch 114: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.62, Accuracy=0.296]\n",
      "Processing Batch 115: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.61, Accuracy=0.298]\n",
      "Processing Batch 116: 100%|██████████| 64/64 [00:34<00:00,  1.85it/s, Loss=2.63, Accuracy=0.293]\n",
      "Processing Batch 117: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.62, Accuracy=0.298]\n",
      "Processing Batch 118: 100%|██████████| 64/64 [00:34<00:00,  1.85it/s, Loss=2.62, Accuracy=0.295]\n",
      "Processing Batch 119: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.61, Accuracy=0.299]\n",
      "Processing Batch 120: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.62, Accuracy=0.296]\n",
      "Processing Batch 121: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.6, Accuracy=0.302] \n",
      "Processing Batch 122: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.62, Accuracy=0.299]\n",
      "Processing Batch 123: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.59, Accuracy=0.304]\n",
      "Processing Batch 124: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.58, Accuracy=0.303]\n",
      "Processing Batch 125: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.6, Accuracy=0.3]   \n",
      "Processing Batch 126: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.59, Accuracy=0.3]  \n",
      "Processing Batch 127: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.6, Accuracy=0.301] \n",
      "Processing Batch 128: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.59, Accuracy=0.303]\n",
      "Processing Batch 129: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.59, Accuracy=0.302]\n",
      "Processing Batch 130: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.6, Accuracy=0.298] \n",
      "Processing Batch 131: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.61, Accuracy=0.298]\n",
      "Processing Batch 132: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.61, Accuracy=0.296]\n",
      "Processing Batch 133: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.59, Accuracy=0.301]\n",
      "Processing Batch 134: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.59, Accuracy=0.303]\n",
      "Processing Batch 135: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.59, Accuracy=0.301]\n",
      "Processing Batch 136: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.6, Accuracy=0.298] \n",
      "Processing Batch 137: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.59, Accuracy=0.305]\n",
      "Processing Batch 138: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.59, Accuracy=0.301]\n",
      "Processing Batch 139: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.59, Accuracy=0.299]\n",
      "Processing Batch 140: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.57, Accuracy=0.304]\n",
      "Processing Batch 141: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.58, Accuracy=0.305]\n",
      "Processing Batch 142: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.6, Accuracy=0.3]   \n",
      "Processing Batch 143: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.58, Accuracy=0.301]\n",
      "Processing Batch 144: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.6, Accuracy=0.299] \n",
      "Processing Batch 145: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.62, Accuracy=0.294]\n",
      "Processing Batch 146: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.62, Accuracy=0.296]\n",
      "Processing Batch 147: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.6, Accuracy=0.3]   \n",
      "Processing Batch 148: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.57, Accuracy=0.304]\n",
      "Processing Batch 149: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.59, Accuracy=0.304]\n",
      "Processing Batch 150: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.56, Accuracy=0.306]\n",
      "Processing Batch 151: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.58, Accuracy=0.305]\n",
      "Processing Batch 152: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.58, Accuracy=0.304]\n",
      "Processing Batch 153: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.59, Accuracy=0.303]\n",
      "Processing Batch 154: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.56, Accuracy=0.305]\n",
      "Processing Batch 155: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.56, Accuracy=0.305]\n",
      "Processing Batch 156: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.55, Accuracy=0.307]\n",
      "Processing Batch 157: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.56, Accuracy=0.306]\n",
      "Processing Batch 158: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.56, Accuracy=0.307]\n",
      "Processing Batch 159: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.55, Accuracy=0.307]\n",
      "Processing Batch 160: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.57, Accuracy=0.303]\n",
      "Processing Batch 161: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.57, Accuracy=0.304]\n",
      "Processing Batch 162: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.56, Accuracy=0.309]\n",
      "Processing Batch 163: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.55, Accuracy=0.311]\n",
      "Processing Batch 164: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.57, Accuracy=0.303]\n",
      "Processing Batch 165: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.55, Accuracy=0.307]\n",
      "Processing Batch 166: 100%|██████████| 64/64 [00:35<00:00,  1.82it/s, Loss=2.55, Accuracy=0.307]\n",
      "Processing Batch 167: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.57, Accuracy=0.305]\n",
      "Processing Batch 168: 100%|██████████| 64/64 [00:35<00:00,  1.80it/s, Loss=2.53, Accuracy=0.31] \n",
      "Processing Batch 169: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.54, Accuracy=0.309]\n",
      "Processing Batch 170: 100%|██████████| 64/64 [00:35<00:00,  1.80it/s, Loss=2.55, Accuracy=0.307]\n",
      "Processing Batch 171: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.53, Accuracy=0.311]\n",
      "Processing Batch 172: 100%|██████████| 64/64 [00:35<00:00,  1.81it/s, Loss=2.52, Accuracy=0.31] \n",
      "Processing Batch 173: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.53, Accuracy=0.311]\n",
      "Processing Batch 174: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.53, Accuracy=0.31] \n",
      "Processing Batch 175: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.58, Accuracy=0.303]\n",
      "Processing Batch 176: 100%|██████████| 64/64 [00:33<00:00,  1.88it/s, Loss=2.55, Accuracy=0.307]\n",
      "Processing Batch 177: 100%|██████████| 64/64 [00:35<00:00,  1.79it/s, Loss=2.56, Accuracy=0.309]\n",
      "Processing Batch 178: 100%|██████████| 64/64 [00:36<00:00,  1.75it/s, Loss=2.56, Accuracy=0.304]\n",
      "Processing Batch 179: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.55, Accuracy=0.309]\n",
      "Processing Batch 180: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.54, Accuracy=0.309]\n",
      "Processing Batch 181: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.54, Accuracy=0.31] \n",
      "Processing Batch 182: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.54, Accuracy=0.311]\n",
      "Processing Batch 183: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.52, Accuracy=0.315]\n",
      "Processing Batch 184: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.53, Accuracy=0.311]\n",
      "Processing Batch 185: 100%|██████████| 64/64 [00:34<00:00,  1.83it/s, Loss=2.53, Accuracy=0.311]\n",
      "Processing Batch 186: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.53, Accuracy=0.312]\n",
      "Processing Batch 187: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.52, Accuracy=0.31] \n",
      "Processing Batch 188: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.53, Accuracy=0.308]\n",
      "Processing Batch 189: 100%|██████████| 64/64 [00:35<00:00,  1.81it/s, Loss=2.53, Accuracy=0.31] \n",
      "Processing Batch 190: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.52, Accuracy=0.312]\n",
      "Processing Batch 191: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.54, Accuracy=0.31] \n",
      "Processing Batch 192: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.54, Accuracy=0.31] \n",
      "Processing Batch 193: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.53, Accuracy=0.31] \n",
      "Processing Batch 194: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.51, Accuracy=0.314]\n",
      "Processing Batch 195: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.51, Accuracy=0.313]\n",
      "Processing Batch 196: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.52, Accuracy=0.314]\n",
      "Processing Batch 197: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.52, Accuracy=0.314]\n",
      "Processing Batch 198: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.51, Accuracy=0.312]\n",
      "Processing Batch 199: 100%|██████████| 64/64 [00:35<00:00,  1.78it/s, Loss=2.53, Accuracy=0.309]\n",
      "Processing Batch 200: 100%|██████████| 64/64 [00:35<00:00,  1.81it/s, Loss=2.51, Accuracy=0.312]\n",
      "Processing Batch 201: 100%|██████████| 64/64 [00:35<00:00,  1.81it/s, Loss=2.52, Accuracy=0.313]\n",
      "Processing Batch 202: 100%|██████████| 64/64 [00:34<00:00,  1.83it/s, Loss=2.52, Accuracy=0.309]\n",
      "Processing Batch 203: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.51, Accuracy=0.313]\n",
      "Processing Batch 204: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.52, Accuracy=0.31] \n",
      "Processing Batch 205: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.53, Accuracy=0.311]\n",
      "Processing Batch 206: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.51, Accuracy=0.313]\n",
      "Processing Batch 207: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.53, Accuracy=0.311]\n",
      "Processing Batch 208: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.52, Accuracy=0.312]\n",
      "Processing Batch 209: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.51, Accuracy=0.315]\n",
      "Processing Batch 210: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.51, Accuracy=0.317]\n",
      "Processing Batch 211: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.51, Accuracy=0.313]\n",
      "Processing Batch 212: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.51, Accuracy=0.312]\n",
      "Processing Batch 213: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.51, Accuracy=0.315]\n",
      "Processing Batch 214: 100%|██████████| 64/64 [00:34<00:00,  1.85it/s, Loss=2.5, Accuracy=0.312] \n",
      "Processing Batch 215: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.52, Accuracy=0.313]\n",
      "Processing Batch 216: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.51, Accuracy=0.312]\n",
      "Processing Batch 217: 100%|██████████| 64/64 [00:35<00:00,  1.82it/s, Loss=2.5, Accuracy=0.316] \n",
      "Processing Batch 218: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.49, Accuracy=0.316]\n",
      "Processing Batch 219: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.51, Accuracy=0.314]\n",
      "Processing Batch 220: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.48, Accuracy=0.318]\n",
      "Processing Batch 221: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.54, Accuracy=0.308]\n",
      "Processing Batch 222: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.51, Accuracy=0.316]\n",
      "Processing Batch 223: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.49, Accuracy=0.316]\n",
      "Processing Batch 224: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.51, Accuracy=0.314]\n",
      "Processing Batch 225: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.51, Accuracy=0.312]\n",
      "Processing Batch 226: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.5, Accuracy=0.315] \n",
      "Processing Batch 227: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.52, Accuracy=0.31] \n",
      "Processing Batch 228: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.48, Accuracy=0.318]\n",
      "Processing Batch 229: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.49, Accuracy=0.317]\n",
      "Processing Batch 230: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.49, Accuracy=0.316]\n",
      "Processing Batch 231: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.48, Accuracy=0.318]\n",
      "Processing Batch 232: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.48, Accuracy=0.321]\n",
      "Processing Batch 233: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.47, Accuracy=0.318]\n",
      "Processing Batch 234: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.49, Accuracy=0.316]\n",
      "Processing Batch 235: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.5, Accuracy=0.318] \n",
      "Processing Batch 236: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.49, Accuracy=0.317]\n",
      "Processing Batch 237: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.51, Accuracy=0.316]\n",
      "Processing Batch 238: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.5, Accuracy=0.313] \n",
      "Processing Batch 239: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.51, Accuracy=0.314]\n",
      "Processing Batch 240: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.49, Accuracy=0.32] \n",
      "Processing Batch 241: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.51, Accuracy=0.314]\n",
      "Processing Batch 242: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.49, Accuracy=0.317]\n",
      "Processing Batch 243: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.48, Accuracy=0.32] \n",
      "Processing Batch 244: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.48, Accuracy=0.32] \n",
      "Processing Batch 245: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.49, Accuracy=0.318]\n",
      "Processing Batch 246: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.49, Accuracy=0.318]\n",
      "Processing Batch 247: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.48, Accuracy=0.315]\n",
      "Processing Batch 248: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.46, Accuracy=0.323]\n",
      "Processing Batch 249: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.47, Accuracy=0.316]\n",
      "Processing Batch 250: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.47, Accuracy=0.32] \n",
      "Processing Batch 251: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.5, Accuracy=0.317] \n",
      "Processing Batch 252: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.51, Accuracy=0.315]\n",
      "Processing Batch 253: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.51, Accuracy=0.315]\n",
      "Processing Batch 254: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.49, Accuracy=0.319]\n",
      "Processing Batch 255: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.49, Accuracy=0.318]\n",
      "Processing Batch 256: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.48, Accuracy=0.315]\n",
      "Processing Batch 257: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.51, Accuracy=0.313]\n",
      "Processing Batch 258: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.49, Accuracy=0.317]\n",
      "Processing Batch 259: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.47, Accuracy=0.319]\n",
      "Processing Batch 260: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.49, Accuracy=0.318]\n",
      "Processing Batch 261: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.47, Accuracy=0.32] \n",
      "Processing Batch 262: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.46, Accuracy=0.32] \n",
      "Processing Batch 263: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.47, Accuracy=0.316]\n",
      "Processing Batch 264: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.47, Accuracy=0.319]\n",
      "Processing Batch 265: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.49, Accuracy=0.317]\n",
      "Processing Batch 266: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.48, Accuracy=0.318]\n",
      "Processing Batch 267: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.47, Accuracy=0.322]\n",
      "Processing Batch 268: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.47, Accuracy=0.321]\n",
      "Processing Batch 269: 100%|██████████| 64/64 [00:30<00:00,  2.12it/s, Loss=2.53, Accuracy=0.309]\n",
      "Processing Batch 270: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.47, Accuracy=0.318]\n",
      "Processing Batch 271: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.47, Accuracy=0.318]\n",
      "Processing Batch 272: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.47, Accuracy=0.319]\n",
      "Processing Batch 273: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.46, Accuracy=0.318]\n",
      "Processing Batch 274: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.47, Accuracy=0.32] \n",
      "Processing Batch 275: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.45, Accuracy=0.322]\n",
      "Processing Batch 276: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.46, Accuracy=0.323]\n",
      "Processing Batch 277: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.46, Accuracy=0.321]\n",
      "Processing Batch 278: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.45, Accuracy=0.323]\n",
      "Processing Batch 279: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.46, Accuracy=0.32] \n",
      "Processing Batch 280: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.46, Accuracy=0.322]\n",
      "Processing Batch 281: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.48, Accuracy=0.316]\n",
      "Processing Batch 282: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.48, Accuracy=0.318]\n",
      "Processing Batch 283: 100%|██████████| 64/64 [00:29<00:00,  2.17it/s, Loss=2.48, Accuracy=0.321]\n",
      "Processing Batch 284: 100%|██████████| 64/64 [00:30<00:00,  2.12it/s, Loss=2.46, Accuracy=0.321]\n",
      "Processing Batch 285: 100%|██████████| 64/64 [00:30<00:00,  2.13it/s, Loss=2.47, Accuracy=0.321]\n",
      "Processing Batch 286: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.45, Accuracy=0.323]\n",
      "Processing Batch 287: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.46, Accuracy=0.321]\n",
      "Processing Batch 288: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 289: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.44, Accuracy=0.322]\n",
      "Processing Batch 290: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 291: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.45, Accuracy=0.326]\n",
      "Processing Batch 292: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.45, Accuracy=0.322]\n",
      "Processing Batch 293: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.46, Accuracy=0.323]\n",
      "Processing Batch 294: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.45, Accuracy=0.321]\n",
      "Processing Batch 295: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.43, Accuracy=0.327]\n",
      "Processing Batch 296: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.46, Accuracy=0.319]\n",
      "Processing Batch 297: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.47, Accuracy=0.318]\n",
      "Processing Batch 298: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.47, Accuracy=0.319]\n",
      "Processing Batch 299: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.42, Accuracy=0.328]\n",
      "Processing Batch 300: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.45, Accuracy=0.323]\n",
      "Processing Batch 301: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.45, Accuracy=0.322]\n",
      "Processing Batch 302: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.45, Accuracy=0.324]\n",
      "Processing Batch 303: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.46, Accuracy=0.324]\n",
      "Processing Batch 304: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.43, Accuracy=0.326]\n",
      "Processing Batch 305: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.45, Accuracy=0.322]\n",
      "Processing Batch 306: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.45, Accuracy=0.323]\n",
      "Processing Batch 307: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 308: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.44, Accuracy=0.323]\n",
      "Processing Batch 309: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.45, Accuracy=0.324]\n",
      "Processing Batch 310: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.44, Accuracy=0.321]\n",
      "Processing Batch 311: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.47, Accuracy=0.319]\n",
      "Processing Batch 312: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.46, Accuracy=0.321]\n",
      "Processing Batch 313: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.43, Accuracy=0.326]\n",
      "Processing Batch 314: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.45, Accuracy=0.323]\n",
      "Processing Batch 315: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.43, Accuracy=0.328]\n",
      "Processing Batch 316: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.44, Accuracy=0.325]\n",
      "Processing Batch 317: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.46, Accuracy=0.32] \n",
      "Processing Batch 318: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.41, Accuracy=0.331]\n",
      "Processing Batch 319: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.43, Accuracy=0.325]\n",
      "Processing Batch 320: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.46, Accuracy=0.321]\n",
      "Processing Batch 321: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.46, Accuracy=0.318]\n",
      "Processing Batch 322: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 323: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.42, Accuracy=0.327]\n",
      "Processing Batch 324: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.42, Accuracy=0.33] \n",
      "Processing Batch 325: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 326: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.46, Accuracy=0.32] \n",
      "Processing Batch 327: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.44, Accuracy=0.325]\n",
      "Processing Batch 328: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.45, Accuracy=0.322]\n",
      "Processing Batch 329: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.43, Accuracy=0.326]\n",
      "Processing Batch 330: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.41, Accuracy=0.333]\n",
      "Processing Batch 331: 100%|██████████| 64/64 [00:30<00:00,  2.12it/s, Loss=2.45, Accuracy=0.325]\n",
      "Processing Batch 332: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.44, Accuracy=0.325]\n",
      "Processing Batch 333: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.45, Accuracy=0.321]\n",
      "Processing Batch 334: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.42, Accuracy=0.326]\n",
      "Processing Batch 335: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.43, Accuracy=0.329]\n",
      "Processing Batch 336: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.45, Accuracy=0.323]\n",
      "Processing Batch 337: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.43, Accuracy=0.323]\n",
      "Processing Batch 338: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.42, Accuracy=0.326]\n",
      "Processing Batch 339: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.45, Accuracy=0.323]\n",
      "Processing Batch 340: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.42, Accuracy=0.328]\n",
      "Processing Batch 341: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.43, Accuracy=0.33] \n",
      "Processing Batch 342: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.43, Accuracy=0.326]\n",
      "Processing Batch 343: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 344: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.45, Accuracy=0.324]\n",
      "Processing Batch 345: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.45, Accuracy=0.318]\n",
      "Processing Batch 346: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.44, Accuracy=0.322]\n",
      "Processing Batch 347: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.41, Accuracy=0.33] \n",
      "Processing Batch 348: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.43, Accuracy=0.326]\n",
      "Processing Batch 349: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.41, Accuracy=0.328]\n",
      "Processing Batch 350: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.41, Accuracy=0.33] \n",
      "Processing Batch 351: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.41, Accuracy=0.33] \n",
      "Processing Batch 352: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.42, Accuracy=0.329]\n",
      "Processing Batch 353: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.42, Accuracy=0.327]\n",
      "Processing Batch 354: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 355: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.42, Accuracy=0.328]\n",
      "Processing Batch 356: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.45, Accuracy=0.322]\n",
      "Processing Batch 357: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.41, Accuracy=0.328]\n",
      "Processing Batch 358: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.43, Accuracy=0.326]\n",
      "Processing Batch 359: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.42, Accuracy=0.331]\n",
      "Processing Batch 360: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.42, Accuracy=0.328]\n",
      "Processing Batch 361: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.41, Accuracy=0.329]\n",
      "Processing Batch 362: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.43, Accuracy=0.328]\n",
      "Processing Batch 363: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.42, Accuracy=0.329]\n",
      "Processing Batch 364: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.43, Accuracy=0.327]\n",
      "Processing Batch 365: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.43, Accuracy=0.326]\n",
      "Processing Batch 366: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.41, Accuracy=0.328]\n",
      "Processing Batch 367: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.41, Accuracy=0.327]\n",
      "Processing Batch 368: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.42, Accuracy=0.327]\n",
      "Processing Batch 369: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.39, Accuracy=0.333]\n",
      "Processing Batch 370: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.41, Accuracy=0.331]\n",
      "Processing Batch 371: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.42, Accuracy=0.328]\n",
      "Processing Batch 372: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.42, Accuracy=0.327]\n",
      "Processing Batch 373: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.42, Accuracy=0.329]\n",
      "Processing Batch 374: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 375: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.44, Accuracy=0.324]\n",
      "Processing Batch 376: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.4, Accuracy=0.332] \n",
      "Processing Batch 377: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.41, Accuracy=0.328]\n",
      "Processing Batch 378: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.41, Accuracy=0.328]\n",
      "Processing Batch 379: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.41, Accuracy=0.329]\n",
      "Processing Batch 380: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.42, Accuracy=0.329]\n",
      "Processing Batch 381: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.4, Accuracy=0.331] \n",
      "Processing Batch 382: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.38, Accuracy=0.334]\n",
      "Processing Batch 383: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.4, Accuracy=0.33]  \n",
      "Processing Batch 384: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.4, Accuracy=0.329] \n",
      "Processing Batch 385: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.4, Accuracy=0.327] \n",
      "Processing Batch 386: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.41, Accuracy=0.331]\n",
      "Processing Batch 387: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.41, Accuracy=0.332]\n",
      "Processing Batch 388: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.43, Accuracy=0.327]\n",
      "Processing Batch 389: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.45, Accuracy=0.323]\n",
      "Processing Batch 390: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.42, Accuracy=0.33] \n",
      "Processing Batch 391: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.41, Accuracy=0.328]\n",
      "Processing Batch 392: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.4, Accuracy=0.33]  \n",
      "Processing Batch 393: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.41, Accuracy=0.329]\n",
      "Processing Batch 394: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.42, Accuracy=0.327]\n",
      "Processing Batch 395: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.42, Accuracy=0.328]\n",
      "Processing Batch 396: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.41, Accuracy=0.33] \n",
      "Processing Batch 397: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.43, Accuracy=0.324]\n",
      "Processing Batch 398: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.43, Accuracy=0.323]\n",
      "Processing Batch 399: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.4, Accuracy=0.332] \n",
      "Processing Batch 400: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.4, Accuracy=0.332] \n",
      "Processing Batch 401: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.41, Accuracy=0.328]\n",
      "Processing Batch 402: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 403: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.42, Accuracy=0.324]\n",
      "Processing Batch 404: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.42, Accuracy=0.324]\n",
      "Processing Batch 405: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.42, Accuracy=0.324]\n",
      "Processing Batch 406: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.4, Accuracy=0.331] \n",
      "Processing Batch 407: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.41, Accuracy=0.327]\n",
      "Processing Batch 408: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.39, Accuracy=0.33] \n",
      "Processing Batch 409: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.4, Accuracy=0.328] \n",
      "Processing Batch 410: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.4, Accuracy=0.331] \n",
      "Processing Batch 411: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.39, Accuracy=0.335]\n",
      "Processing Batch 412: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 413: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.39, Accuracy=0.332]\n",
      "Processing Batch 414: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.39, Accuracy=0.334]\n",
      "Processing Batch 415: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.37, Accuracy=0.336]\n",
      "Processing Batch 416: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.38, Accuracy=0.331]\n",
      "Processing Batch 417: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.4, Accuracy=0.331] \n",
      "Processing Batch 418: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.41, Accuracy=0.329]\n",
      "Processing Batch 419: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 420: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.4, Accuracy=0.331] \n",
      "Processing Batch 421: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.38, Accuracy=0.334]\n",
      "Processing Batch 422: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 423: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.38, Accuracy=0.333]\n",
      "Processing Batch 424: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 425: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.39, Accuracy=0.329]\n",
      "Processing Batch 426: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 427: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 428: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.39, Accuracy=0.332]\n",
      "Processing Batch 429: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.4, Accuracy=0.331] \n",
      "Processing Batch 430: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.4, Accuracy=0.331] \n",
      "Processing Batch 431: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.4, Accuracy=0.334] \n",
      "Processing Batch 432: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 433: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.39, Accuracy=0.333]\n",
      "Processing Batch 434: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 435: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.38, Accuracy=0.335]\n",
      "Processing Batch 436: 100%|██████████| 64/64 [00:30<00:00,  2.06it/s, Loss=2.39, Accuracy=0.332]\n",
      "Processing Batch 437: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.39, Accuracy=0.332]\n",
      "Processing Batch 438: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.38, Accuracy=0.334]\n",
      "Processing Batch 439: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.37, Accuracy=0.336]\n",
      "Processing Batch 440: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.38, Accuracy=0.335]\n",
      "Processing Batch 441: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.39, Accuracy=0.332]\n",
      "Processing Batch 442: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.38, Accuracy=0.333]\n",
      "Processing Batch 443: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.4, Accuracy=0.332] \n",
      "Processing Batch 444: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.38, Accuracy=0.332]\n",
      "Processing Batch 445: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.38, Accuracy=0.333]\n",
      "Processing Batch 446: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.39, Accuracy=0.335]\n",
      "Processing Batch 447: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.39, Accuracy=0.334]\n",
      "Processing Batch 448: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.38, Accuracy=0.332]\n",
      "Processing Batch 449: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.37, Accuracy=0.336]\n",
      "Processing Batch 450: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.38, Accuracy=0.335]\n",
      "Processing Batch 451: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.39, Accuracy=0.333]\n",
      "Processing Batch 452: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.37, Accuracy=0.339]\n",
      "Processing Batch 453: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 454: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 455: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.4, Accuracy=0.329] \n",
      "Processing Batch 456: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.38, Accuracy=0.335]\n",
      "Processing Batch 457: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.38, Accuracy=0.334]\n",
      "Processing Batch 458: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.39, Accuracy=0.333]\n",
      "Processing Batch 459: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.38, Accuracy=0.335]\n",
      "Processing Batch 460: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.4, Accuracy=0.328] \n",
      "Processing Batch 461: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.4, Accuracy=0.331] \n",
      "Processing Batch 462: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.4, Accuracy=0.33]  \n",
      "Processing Batch 463: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.36, Accuracy=0.338]\n",
      "Processing Batch 464: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.39, Accuracy=0.333]\n",
      "Processing Batch 465: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.41, Accuracy=0.329]\n",
      "Processing Batch 466: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.36, Accuracy=0.339]\n",
      "Processing Batch 467: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.37, Accuracy=0.338]\n",
      "Processing Batch 468: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.39, Accuracy=0.334]\n",
      "Processing Batch 469: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.38, Accuracy=0.332]\n",
      "Processing Batch 470: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 471: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.38, Accuracy=0.334]\n",
      "Processing Batch 472: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 473: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.36, Accuracy=0.335]\n",
      "Processing Batch 474: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 475: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.38, Accuracy=0.335]\n",
      "Processing Batch 476: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.39, Accuracy=0.333]\n",
      "Processing Batch 477: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.38, Accuracy=0.335]\n",
      "Processing Batch 478: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 479: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.37, Accuracy=0.336]\n",
      "Processing Batch 480: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.4, Accuracy=0.33]  \n",
      "Processing Batch 481: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.37, Accuracy=0.337]\n",
      "Processing Batch 482: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.39, Accuracy=0.33] \n",
      "Processing Batch 483: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 484: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.38, Accuracy=0.334]\n",
      "Processing Batch 485: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.39, Accuracy=0.332]\n",
      "Processing Batch 486: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s, Loss=2.37, Accuracy=0.338]\n",
      "Processing Batch 487: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s, Loss=2.38, Accuracy=0.336]\n",
      "Processing Batch 488: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.4, Accuracy=0.329] \n",
      "Processing Batch 489: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 490: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.38, Accuracy=0.332]\n",
      "Processing Batch 491: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.38, Accuracy=0.333]\n",
      "Processing Batch 492: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.39, Accuracy=0.333]\n",
      "Processing Batch 493: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.37, Accuracy=0.333]\n",
      "Processing Batch 494: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.37, Accuracy=0.334]\n",
      "Processing Batch 495: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.37, Accuracy=0.336]\n",
      "Processing Batch 496: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.39, Accuracy=0.333]\n",
      "Processing Batch 497: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.4, Accuracy=0.332] \n",
      "Processing Batch 498: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.37, Accuracy=0.337]\n",
      "Processing Batch 499: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 500: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.38, Accuracy=0.331]\n",
      "Processing Batch 501: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 502: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 503: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.37, Accuracy=0.337]\n",
      "Processing Batch 504: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.37, Accuracy=0.337]\n",
      "Processing Batch 505: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.36, Accuracy=0.338]\n",
      "Processing Batch 506: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.37, Accuracy=0.338]\n",
      "Processing Batch 507: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.36, Accuracy=0.338]\n",
      "Processing Batch 508: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.36, Accuracy=0.334]\n",
      "Processing Batch 509: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.37, Accuracy=0.337]\n",
      "Processing Batch 510: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s, Loss=2.36, Accuracy=0.335]\n",
      "Processing Batch 511: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.36, Accuracy=0.335]\n",
      "Processing Batch 512: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.37, Accuracy=0.333]\n",
      "Processing Batch 513: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.37, Accuracy=0.336]\n",
      "Processing Batch 514: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 515: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.36, Accuracy=0.339]\n",
      "Processing Batch 516: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 517: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 518: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.37, Accuracy=0.334]\n",
      "Processing Batch 519: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.36, Accuracy=0.339]\n",
      "Processing Batch 520: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.36, Accuracy=0.342]\n",
      "Processing Batch 521: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.36, Accuracy=0.339]\n",
      "Processing Batch 522: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 523: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.35, Accuracy=0.341]\n",
      "Processing Batch 524: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.35, Accuracy=0.336]\n",
      "Processing Batch 525: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.36, Accuracy=0.335]\n",
      "Processing Batch 526: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.36, Accuracy=0.338]\n",
      "Processing Batch 527: 100%|██████████| 64/64 [00:34<00:00,  1.85it/s, Loss=2.38, Accuracy=0.331]\n",
      "Processing Batch 528: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 529: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.38, Accuracy=0.333]\n",
      "Processing Batch 530: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.38, Accuracy=0.333]\n",
      "Processing Batch 531: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.42, Accuracy=0.326]\n",
      "Processing Batch 532: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.39, Accuracy=0.332]\n",
      "Processing Batch 533: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.38, Accuracy=0.331]\n",
      "Processing Batch 534: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 535: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.35, Accuracy=0.341]\n",
      "Processing Batch 536: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.36, Accuracy=0.335]\n",
      "Processing Batch 537: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.35, Accuracy=0.34] \n",
      "Processing Batch 538: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 539: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 540: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.37, Accuracy=0.334]\n",
      "Processing Batch 541: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.36, Accuracy=0.337]\n",
      "Processing Batch 542: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.36, Accuracy=0.335]\n",
      "Processing Batch 543: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 544: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.36, Accuracy=0.337]\n",
      "Processing Batch 545: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.37, Accuracy=0.334]\n",
      "Processing Batch 546: 100%|██████████| 64/64 [00:36<00:00,  1.77it/s, Loss=2.4, Accuracy=0.329] \n",
      "Processing Batch 547: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 548: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.37, Accuracy=0.334]\n",
      "Processing Batch 549: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.36, Accuracy=0.337]\n",
      "Processing Batch 550: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.33, Accuracy=0.343]\n",
      "Processing Batch 551: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 552: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.35, Accuracy=0.338]\n",
      "Processing Batch 553: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.36, Accuracy=0.338]\n",
      "Processing Batch 554: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 555: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 556: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.38, Accuracy=0.333]\n",
      "Processing Batch 557: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 558: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 559: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 560: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 561: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.37, Accuracy=0.334]\n",
      "Processing Batch 562: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.37, Accuracy=0.338]\n",
      "Processing Batch 563: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.34, Accuracy=0.342]\n",
      "Processing Batch 564: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 565: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.34, Accuracy=0.344]\n",
      "Processing Batch 566: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.34, Accuracy=0.344]\n",
      "Processing Batch 567: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 568: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.34, Accuracy=0.342]\n",
      "Processing Batch 569: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.36, Accuracy=0.338]\n",
      "Processing Batch 570: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.36, Accuracy=0.337]\n",
      "Processing Batch 571: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 572: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 573: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.33, Accuracy=0.339]\n",
      "Processing Batch 574: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.34, Accuracy=0.337]\n",
      "Processing Batch 575: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 576: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.35, Accuracy=0.338]\n",
      "Processing Batch 577: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.35, Accuracy=0.34] \n",
      "Processing Batch 578: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.35, Accuracy=0.341]\n",
      "Processing Batch 579: 100%|██████████| 64/64 [00:41<00:00,  1.56it/s, Loss=2.43, Accuracy=0.323]\n",
      "Processing Batch 580: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 581: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 582: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.34, Accuracy=0.343]\n",
      "Processing Batch 583: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 584: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.34, Accuracy=0.338]\n",
      "Processing Batch 585: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.35, Accuracy=0.337]\n",
      "Processing Batch 586: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 587: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 588: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.36, Accuracy=0.337]\n",
      "Processing Batch 589: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.34, Accuracy=0.342]\n",
      "Processing Batch 590: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 591: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 592: 100%|██████████| 64/64 [00:35<00:00,  1.80it/s, Loss=2.39, Accuracy=0.331]\n",
      "Processing Batch 593: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 594: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.35, Accuracy=0.337]\n",
      "Processing Batch 595: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.36, Accuracy=0.338]\n",
      "Processing Batch 596: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.34, Accuracy=0.344]\n",
      "Processing Batch 597: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.35, Accuracy=0.338]\n",
      "Processing Batch 598: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 599: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 600: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.34, Accuracy=0.342]\n",
      "Processing Batch 601: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 602: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.33, Accuracy=0.343]\n",
      "Processing Batch 603: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 604: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.34, Accuracy=0.338]\n",
      "Processing Batch 605: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 606: 100%|██████████| 64/64 [00:33<00:00,  1.88it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 607: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 608: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.33, Accuracy=0.339]\n",
      "Processing Batch 609: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.35, Accuracy=0.336]\n",
      "Processing Batch 610: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.35, Accuracy=0.335]\n",
      "Processing Batch 611: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.33, Accuracy=0.343]\n",
      "Processing Batch 612: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 613: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.36, Accuracy=0.337]\n",
      "Processing Batch 614: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 615: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 616: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.35, Accuracy=0.337]\n",
      "Processing Batch 617: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.343]\n",
      "Processing Batch 618: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.34, Accuracy=0.338]\n",
      "Processing Batch 619: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 620: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 621: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 622: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.35, Accuracy=0.34] \n",
      "Processing Batch 623: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.32, Accuracy=0.343]\n",
      "Processing Batch 624: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.33, Accuracy=0.344]\n",
      "Processing Batch 625: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 626: 100%|██████████| 64/64 [00:30<00:00,  2.09it/s, Loss=2.32, Accuracy=0.342]\n",
      "Processing Batch 627: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.37, Accuracy=0.335]\n",
      "Processing Batch 628: 100%|██████████| 64/64 [00:35<00:00,  1.81it/s, Loss=2.37, Accuracy=0.332]\n",
      "Processing Batch 629: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 630: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 631: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 632: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 633: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 634: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 635: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 636: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 637: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 638: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.34, Accuracy=0.34] \n",
      "Processing Batch 639: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.32, Accuracy=0.343]\n",
      "Processing Batch 640: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.35, Accuracy=0.337]\n",
      "Processing Batch 641: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 642: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.34, Accuracy=0.338]\n",
      "Processing Batch 643: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 644: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.37, Accuracy=0.336]\n",
      "Processing Batch 645: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.35, Accuracy=0.341]\n",
      "Processing Batch 646: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.32, Accuracy=0.342]\n",
      "Processing Batch 647: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.32, Accuracy=0.342]\n",
      "Processing Batch 648: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 649: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.3, Accuracy=0.349] \n",
      "Processing Batch 650: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 651: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 652: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.33, Accuracy=0.345]\n",
      "Processing Batch 653: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.34, Accuracy=0.342]\n",
      "Processing Batch 654: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 655: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 656: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 657: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.36, Accuracy=0.337]\n",
      "Processing Batch 658: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.35, Accuracy=0.335]\n",
      "Processing Batch 659: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.33, Accuracy=0.344]\n",
      "Processing Batch 660: 100%|██████████| 64/64 [00:30<00:00,  2.12it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 661: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.32, Accuracy=0.345]\n",
      "Processing Batch 662: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.34, Accuracy=0.34] \n",
      "Processing Batch 663: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.36, Accuracy=0.335]\n",
      "Processing Batch 664: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 665: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.35, Accuracy=0.334]\n",
      "Processing Batch 666: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.34, Accuracy=0.338]\n",
      "Processing Batch 667: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.35, Accuracy=0.34] \n",
      "Processing Batch 668: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.35, Accuracy=0.336]\n",
      "Processing Batch 669: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 670: 100%|██████████| 64/64 [00:33<00:00,  1.88it/s, Loss=2.37, Accuracy=0.334]\n",
      "Processing Batch 671: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 672: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.35, Accuracy=0.337]\n",
      "Processing Batch 673: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 674: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 675: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.33, Accuracy=0.34] \n",
      "Processing Batch 676: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 677: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 678: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 679: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.33, Accuracy=0.34] \n",
      "Processing Batch 680: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.32, Accuracy=0.341]\n",
      "Processing Batch 681: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.32, Accuracy=0.347]\n",
      "Processing Batch 682: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.33, Accuracy=0.34] \n",
      "Processing Batch 683: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 684: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.34, Accuracy=0.337]\n",
      "Processing Batch 685: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.33, Accuracy=0.343]\n",
      "Processing Batch 686: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.35, Accuracy=0.338]\n",
      "Processing Batch 687: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.35, Accuracy=0.337]\n",
      "Processing Batch 688: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.32, Accuracy=0.346]\n",
      "Processing Batch 689: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 690: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 691: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.32, Accuracy=0.343]\n",
      "Processing Batch 692: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 693: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.32, Accuracy=0.34] \n",
      "Processing Batch 694: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 695: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.33, Accuracy=0.342]\n",
      "Processing Batch 696: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 697: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 698: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.33, Accuracy=0.338]\n",
      "Processing Batch 699: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.32, Accuracy=0.343]\n",
      "Processing Batch 700: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 701: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 702: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.36, Accuracy=0.336]\n",
      "Processing Batch 703: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.33, Accuracy=0.344]\n",
      "Processing Batch 704: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.33, Accuracy=0.344]\n",
      "Processing Batch 705: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.33, Accuracy=0.345]\n",
      "Processing Batch 706: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.32, Accuracy=0.345]\n",
      "Processing Batch 707: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.33, Accuracy=0.343]\n",
      "Processing Batch 708: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.33, Accuracy=0.344]\n",
      "Processing Batch 709: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 710: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.35, Accuracy=0.339]\n",
      "Processing Batch 711: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 712: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.3, Accuracy=0.344] \n",
      "Processing Batch 713: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 714: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 715: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.33, Accuracy=0.34] \n",
      "Processing Batch 716: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.32, Accuracy=0.343]\n",
      "Processing Batch 717: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.34, Accuracy=0.338]\n",
      "Processing Batch 718: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.34, Accuracy=0.341]\n",
      "Processing Batch 719: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.35, Accuracy=0.34] \n",
      "Processing Batch 720: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 721: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.31, Accuracy=0.348]\n",
      "Processing Batch 722: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 723: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.32, Accuracy=0.342]\n",
      "Processing Batch 724: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.32, Accuracy=0.343]\n",
      "Processing Batch 725: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.32, Accuracy=0.345]\n",
      "Processing Batch 726: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 727: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.32, Accuracy=0.345]\n",
      "Processing Batch 728: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 729: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 730: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 731: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 732: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 733: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 734: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.34, Accuracy=0.339]\n",
      "Processing Batch 735: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.34, Accuracy=0.34] \n",
      "Processing Batch 736: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.32, Accuracy=0.345]\n",
      "Processing Batch 737: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.33, Accuracy=0.343]\n",
      "Processing Batch 738: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 739: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 740: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 741: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.34, Accuracy=0.34] \n",
      "Processing Batch 742: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.3, Accuracy=0.35]  \n",
      "Processing Batch 743: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 744: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 745: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 746: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 747: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.32, Accuracy=0.346]\n",
      "Processing Batch 748: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 749: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 750: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.33, Accuracy=0.345]\n",
      "Processing Batch 751: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.32, Accuracy=0.345]\n",
      "Processing Batch 752: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 753: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.31, Accuracy=0.342]\n",
      "Processing Batch 754: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.31, Accuracy=0.348]\n",
      "Processing Batch 755: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.32, Accuracy=0.346]\n",
      "Processing Batch 756: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 757: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.31, Accuracy=0.343]\n",
      "Processing Batch 758: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 759: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 760: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 761: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 762: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 763: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 764: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.34, Accuracy=0.34] \n",
      "Processing Batch 765: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.3, Accuracy=0.35]  \n",
      "Processing Batch 766: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.3, Accuracy=0.349] \n",
      "Processing Batch 767: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 768: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 769: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 770: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.3, Accuracy=0.349] \n",
      "Processing Batch 771: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 772: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 773: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 774: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 775: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.3, Accuracy=0.345] \n",
      "Processing Batch 776: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.33, Accuracy=0.34] \n",
      "Processing Batch 777: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.33, Accuracy=0.343]\n",
      "Processing Batch 778: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 779: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 780: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 781: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 782: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 783: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 784: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.28, Accuracy=0.347]\n",
      "Processing Batch 785: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.3, Accuracy=0.344] \n",
      "Processing Batch 786: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 787: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 788: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 789: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 790: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 791: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 792: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 793: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.3, Accuracy=0.349] \n",
      "Processing Batch 794: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 795: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 796: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s, Loss=2.29, Accuracy=0.351]\n",
      "Processing Batch 797: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 798: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 799: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 800: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 801: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 802: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 803: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 804: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 805: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.31, Accuracy=0.348]\n",
      "Processing Batch 806: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 807: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 808: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 809: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 810: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.29, Accuracy=0.346]\n",
      "Processing Batch 811: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 812: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 813: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 814: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.32, Accuracy=0.345]\n",
      "Processing Batch 815: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 816: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 817: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.33, Accuracy=0.34] \n",
      "Processing Batch 818: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 819: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 820: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 821: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 822: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.3, Accuracy=0.349] \n",
      "Processing Batch 823: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 824: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 825: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.29, Accuracy=0.353]\n",
      "Processing Batch 826: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.29, Accuracy=0.346]\n",
      "Processing Batch 827: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.3, Accuracy=0.349] \n",
      "Processing Batch 828: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.28, Accuracy=0.348]\n",
      "Processing Batch 829: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.29, Accuracy=0.347]\n",
      "Processing Batch 830: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.29, Accuracy=0.347]\n",
      "Processing Batch 831: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 832: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 833: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 834: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.32, Accuracy=0.344]\n",
      "Processing Batch 835: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.31, Accuracy=0.348]\n",
      "Processing Batch 836: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 837: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.28, Accuracy=0.352]\n",
      "Processing Batch 838: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 839: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 840: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 841: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 842: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 843: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 844: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 845: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.3, Accuracy=0.343] \n",
      "Processing Batch 846: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.31, Accuracy=0.342]\n",
      "Processing Batch 847: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 848: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.32, Accuracy=0.343]\n",
      "Processing Batch 849: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.3, Accuracy=0.349] \n",
      "Processing Batch 850: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 851: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 852: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 853: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.29, Accuracy=0.351]\n",
      "Processing Batch 854: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.3, Accuracy=0.345] \n",
      "Processing Batch 855: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 856: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 857: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 858: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 859: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.3, Accuracy=0.344] \n",
      "Processing Batch 860: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.351]\n",
      "Processing Batch 861: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 862: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 863: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 864: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.28, Accuracy=0.352]\n",
      "Processing Batch 865: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 866: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 867: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.28, Accuracy=0.346]\n",
      "Processing Batch 868: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 869: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 870: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 871: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 872: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.28, Accuracy=0.348]\n",
      "Processing Batch 873: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.29, Accuracy=0.347]\n",
      "Processing Batch 874: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.31, Accuracy=0.344]\n",
      "Processing Batch 875: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.31, Accuracy=0.348]\n",
      "Processing Batch 876: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 877: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 878: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 879: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 880: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 881: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 882: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 883: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 884: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 885: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 886: 100%|██████████| 64/64 [00:34<00:00,  1.85it/s, Loss=2.29, Accuracy=0.346]\n",
      "Processing Batch 887: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 888: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 889: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 890: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 891: 100%|██████████| 64/64 [00:34<00:00,  1.88it/s, Loss=2.33, Accuracy=0.341]\n",
      "Processing Batch 892: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.3, Accuracy=0.35]  \n",
      "Processing Batch 893: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 894: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 895: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 896: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 897: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 898: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 899: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 900: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 901: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 902: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 903: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 904: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 905: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 906: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.31, Accuracy=0.346]\n",
      "Processing Batch 907: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 908: 100%|██████████| 64/64 [00:32<00:00,  2.00it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 909: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 910: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.27, Accuracy=0.356]\n",
      "Processing Batch 911: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 912: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 913: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 914: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 915: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.26, Accuracy=0.356]\n",
      "Processing Batch 916: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 917: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.348]\n",
      "Processing Batch 918: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.29, Accuracy=0.347]\n",
      "Processing Batch 919: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.32, Accuracy=0.345]\n",
      "Processing Batch 920: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 921: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 922: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 923: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 924: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.29, Accuracy=0.352]\n",
      "Processing Batch 925: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 926: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.28, Accuracy=0.352]\n",
      "Processing Batch 927: 100%|██████████| 64/64 [00:31<00:00,  2.00it/s, Loss=2.27, Accuracy=0.354]\n",
      "Processing Batch 928: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 929: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.26, Accuracy=0.355]\n",
      "Processing Batch 930: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 931: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 932: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 933: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 934: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 935: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 936: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 937: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 938: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 939: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 940: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 941: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 942: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 943: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 944: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 945: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 946: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.27, Accuracy=0.348]\n",
      "Processing Batch 947: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.27, Accuracy=0.349]\n",
      "Processing Batch 948: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.28, Accuracy=0.347]\n",
      "Processing Batch 949: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.3, Accuracy=0.346] \n",
      "Processing Batch 950: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 951: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 952: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.26, Accuracy=0.353]\n",
      "Processing Batch 953: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 954: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 955: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 956: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.352]\n",
      "Processing Batch 957: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 958: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 959: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 960: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 961: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 962: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 963: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 964: 100%|██████████| 64/64 [00:33<00:00,  1.89it/s, Loss=2.29, Accuracy=0.346]\n",
      "Processing Batch 965: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 966: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 967: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 968: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.27, Accuracy=0.354]\n",
      "Processing Batch 969: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.27, Accuracy=0.35] \n",
      "Processing Batch 970: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.27, Accuracy=0.355]\n",
      "Processing Batch 971: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 972: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 973: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 974: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 975: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.29, Accuracy=0.347]\n",
      "Processing Batch 976: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 977: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.29, Accuracy=0.351]\n",
      "Processing Batch 978: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.27, Accuracy=0.354]\n",
      "Processing Batch 979: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s, Loss=2.25, Accuracy=0.357]\n",
      "Processing Batch 980: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 981: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.28, Accuracy=0.352]\n",
      "Processing Batch 982: 100%|██████████| 64/64 [00:31<00:00,  2.04it/s, Loss=2.29, Accuracy=0.351]\n",
      "Processing Batch 983: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.26, Accuracy=0.355]\n",
      "Processing Batch 984: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 985: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.25, Accuracy=0.357]\n",
      "Processing Batch 986: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.27, Accuracy=0.349]\n",
      "Processing Batch 987: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.26, Accuracy=0.353]\n",
      "Processing Batch 988: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 989: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.25, Accuracy=0.357]\n",
      "Processing Batch 990: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.29, Accuracy=0.351]\n",
      "Processing Batch 991: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 992: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 993: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 994: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 995: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.27, Accuracy=0.354]\n",
      "Processing Batch 996: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 997: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.26, Accuracy=0.356]\n",
      "Processing Batch 998: 100%|██████████| 64/64 [00:31<00:00,  2.02it/s, Loss=2.26, Accuracy=0.351]\n",
      "Processing Batch 999: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 1000: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 1001: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 1002: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.26, Accuracy=0.356]\n",
      "Processing Batch 1003: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 1004: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.29, Accuracy=0.35] \n",
      "Processing Batch 1005: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.31, Accuracy=0.347]\n",
      "Processing Batch 1006: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.352]\n",
      "Processing Batch 1007: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 1008: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 1009: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.29, Accuracy=0.347]\n",
      "Processing Batch 1010: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.27, Accuracy=0.35] \n",
      "Processing Batch 1011: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.26, Accuracy=0.353]\n",
      "Processing Batch 1012: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.27, Accuracy=0.35] \n",
      "Processing Batch 1013: 100%|██████████| 64/64 [00:35<00:00,  1.82it/s, Loss=2.28, Accuracy=0.348]\n",
      "Processing Batch 1014: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 1015: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.27, Accuracy=0.35] \n",
      "Processing Batch 1016: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.29, Accuracy=0.347]\n",
      "Processing Batch 1017: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 1018: 100%|██████████| 64/64 [00:34<00:00,  1.86it/s, Loss=2.29, Accuracy=0.345]\n",
      "Processing Batch 1019: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 1020: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 1021: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 1022: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.26, Accuracy=0.353]\n",
      "Processing Batch 1023: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 1024: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 1025: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.27, Accuracy=0.354]\n",
      "Processing Batch 1026: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 1027: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.27, Accuracy=0.349]\n",
      "Processing Batch 1028: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.25, Accuracy=0.356]\n",
      "Processing Batch 1029: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.25, Accuracy=0.356]\n",
      "Processing Batch 1030: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.27, Accuracy=0.354]\n",
      "Processing Batch 1031: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 1032: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 1033: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 1034: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.3, Accuracy=0.348] \n",
      "Processing Batch 1035: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 1036: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.26, Accuracy=0.354]\n",
      "Processing Batch 1037: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.29, Accuracy=0.349]\n",
      "Processing Batch 1038: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.26, Accuracy=0.353]\n",
      "Processing Batch 1039: 100%|██████████| 64/64 [00:31<00:00,  2.01it/s, Loss=2.25, Accuracy=0.355]\n",
      "Processing Batch 1040: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.27, Accuracy=0.35] \n",
      "Processing Batch 1041: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 1042: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 1043: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 1044: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.28, Accuracy=0.347]\n",
      "Processing Batch 1045: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.28, Accuracy=0.349]\n",
      "Processing Batch 1046: 100%|██████████| 64/64 [00:33<00:00,  1.88it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 1047: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 1048: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.31, Accuracy=0.345]\n",
      "Processing Batch 1049: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.26, Accuracy=0.355]\n",
      "Processing Batch 1050: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.28, Accuracy=0.352]\n",
      "Processing Batch 1051: 100%|██████████| 64/64 [00:32<00:00,  1.99it/s, Loss=2.25, Accuracy=0.356]\n",
      "Processing Batch 1052: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 1053: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.27, Accuracy=0.349]\n",
      "Processing Batch 1054: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s, Loss=2.26, Accuracy=0.356]\n",
      "Processing Batch 1055: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.25, Accuracy=0.355]\n",
      "Processing Batch 1056: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.26, Accuracy=0.353]\n",
      "Processing Batch 1057: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.26, Accuracy=0.352]\n",
      "Processing Batch 1058: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.25, Accuracy=0.356]\n",
      "Processing Batch 1059: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 1060: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 1061: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.28, Accuracy=0.353]\n",
      "Processing Batch 1062: 100%|██████████| 64/64 [00:33<00:00,  1.91it/s, Loss=2.29, Accuracy=0.348]\n",
      "Processing Batch 1063: 100%|██████████| 64/64 [00:33<00:00,  1.94it/s, Loss=2.28, Accuracy=0.347]\n",
      "Processing Batch 1064: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.26, Accuracy=0.354]\n",
      "Processing Batch 1065: 100%|██████████| 64/64 [00:33<00:00,  1.90it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 1066: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.26, Accuracy=0.354]\n",
      "Processing Batch 1067: 100%|██████████| 64/64 [00:32<00:00,  1.94it/s, Loss=2.28, Accuracy=0.348]\n",
      "Processing Batch 1068: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 1069: 100%|██████████| 64/64 [00:34<00:00,  1.87it/s, Loss=2.26, Accuracy=0.351]\n",
      "Processing Batch 1070: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.24, Accuracy=0.356]\n",
      "Processing Batch 1071: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s, Loss=2.26, Accuracy=0.351]\n",
      "Processing Batch 1072: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.25, Accuracy=0.358]\n",
      "Processing Batch 1073: 100%|██████████| 64/64 [00:35<00:00,  1.81it/s, Loss=2.27, Accuracy=0.353]\n",
      "Processing Batch 1074: 100%|██████████| 64/64 [00:34<00:00,  1.84it/s, Loss=2.3, Accuracy=0.347] \n",
      "Processing Batch 1075: 100%|██████████| 64/64 [00:34<00:00,  1.85it/s, Loss=2.3, Accuracy=0.345] \n",
      "Processing Batch 1076: 100%|██████████| 64/64 [00:32<00:00,  1.95it/s, Loss=2.28, Accuracy=0.348]\n",
      "Processing Batch 1077: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s, Loss=2.28, Accuracy=0.35] \n",
      "Processing Batch 1078: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.26, Accuracy=0.351]\n",
      "Processing Batch 1079: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s, Loss=2.28, Accuracy=0.351]\n",
      "Processing Batch 1080: 100%|██████████| 64/64 [00:32<00:00,  1.96it/s, Loss=2.26, Accuracy=0.353]\n",
      "Processing Batch 1081: 100%|██████████| 64/64 [00:32<00:00,  1.97it/s, Loss=2.27, Accuracy=0.352]\n",
      "Processing Batch 1082: 100%|██████████| 64/64 [00:32<00:00,  1.98it/s, Loss=2.27, Accuracy=0.351]\n",
      "Processing Batch 1083:  33%|███▎      | 21/64 [00:11<00:22,  1.89it/s, Loss=2.26, Accuracy=0.357]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    107\u001b[0m game_batch \u001b[38;5;241m=\u001b[39m [game\u001b[38;5;241m.\u001b[39mpgn \u001b[38;5;28;01mfor\u001b[39;00m game \u001b[38;5;129;01min\u001b[39;00m games[i:i \u001b[38;5;241m+\u001b[39m game_batch_size]]\n\u001b[1;32m--> 108\u001b[0m loss, accuracy, moves \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_moves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m moves\n\u001b[0;32m    110\u001b[0m total_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy \u001b[38;5;241m*\u001b[39m moves\n",
      "Cell \u001b[1;32mIn[11], line 66\u001b[0m, in \u001b[0;36mtrain_on_batch\u001b[1;34m(games, model, optimizer, criterion, device, skip_moves)\u001b[0m\n\u001b[0;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m model(batch_inputs)\n\u001b[0;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_targets)\n\u001b[1;32m---> 66\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mcalculate_accuracy\u001b[1;34m(output, target)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_accuracy\u001b[39m(output, target):\n\u001b[0;32m     20\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     correct \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def board_to_input(board):\n",
    "    board_planes = np.zeros((8, 8, 12), dtype=np.float32)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            plane = piece.piece_type - 1\n",
    "            if piece.color == chess.BLACK:\n",
    "                plane += 6\n",
    "            row, col = divmod(square, 8)\n",
    "            board_planes[row, col, plane] = 1\n",
    "    return board_planes\n",
    "\n",
    "#Encode the move TODO: Make it possible to encode promotion moves. ATM it only encodes the move from and to square and is unable to promote.\n",
    "def move_to_output(move):\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    return from_square * 64 + to_square\n",
    "\n",
    "def calculate_accuracy(output, target):\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    correct = (predicted == target).sum().item()\n",
    "    return correct / target.size(0)\n",
    "\n",
    "#Training step with move skipping and batching\n",
    "def train_on_batch(games, model, optimizer, criterion, device, skip_moves=10):\n",
    "    all_board_inputs = []\n",
    "    all_targets = []\n",
    "    total_moves = 0\n",
    "\n",
    "    for game_str in games:\n",
    "        pgn_io = io.StringIO(game_str)\n",
    "        game = chess.pgn.read_game(pgn_io)\n",
    "        board = game.board()\n",
    "        move_count = 0\n",
    "\n",
    "        for move in game.mainline_moves():\n",
    "            #A static number of moves are skipped to avoid overfitting to the opening book\n",
    "            #More sophisticated methods can be used to skip exact amount of book moves, but it is too inefficient for my machine\n",
    "            if move_count < skip_moves:\n",
    "                board.push(move)\n",
    "                move_count += 1\n",
    "                continue\n",
    "\n",
    "            #Prepare the input and output\n",
    "            board_input = board_to_input(board)\n",
    "            board_input = torch.tensor(board_input, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "            actual_output = move_to_output(move)\n",
    "            actual_output = torch.tensor([actual_output], dtype=torch.long).to(device)\n",
    "\n",
    "            all_board_inputs.append(board_input)\n",
    "            all_targets.append(actual_output)\n",
    "            total_moves += 1\n",
    "\n",
    "            #Update the board with the actual move\n",
    "            board.push(move)\n",
    "\n",
    "    if all_board_inputs:\n",
    "        #Stack all inputs and targets\n",
    "        batch_inputs = torch.cat(all_board_inputs, dim=0)\n",
    "        batch_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_inputs)\n",
    "\n",
    "        loss = criterion(output, batch_targets)\n",
    "        accuracy = calculate_accuracy(output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        del batch_inputs, batch_targets, output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return loss.item(), accuracy, total_moves\n",
    "    else:\n",
    "        return 0, 0, 0  #If no valid moves in batch\n",
    "\n",
    "#Training loop\n",
    "batch_size = 1024\n",
    "game_batch_size = 16 \n",
    "offset = 0\n",
    "step = 0\n",
    "j = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ChessNet().to(device)\n",
    "model.load_state_dict(torch.load('savedModels/cnn_transformer_model_current.pth')) #Load the model from the previous training session\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "while True:\n",
    "    games = session.query(ChessGame).offset(offset).limit(batch_size).all()\n",
    "    if not games:\n",
    "        break\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_moves = 0\n",
    "    \n",
    "\n",
    "    with tqdm(total=len(games) // game_batch_size, desc=f\"Processing Batch {offset // batch_size + 1}\") as pbar:\n",
    "        \n",
    "        for i in range(0, len(games), game_batch_size):\n",
    "            if offset // batch_size + 1 <63:\n",
    "                break\n",
    "            game_batch = [game.pgn for game in games[i:i + game_batch_size]]\n",
    "            loss, accuracy, moves = train_on_batch(game_batch, model, optimizer, criterion, device, skip_moves=10)\n",
    "            total_loss += loss * moves\n",
    "            total_accuracy += accuracy * moves\n",
    "            total_moves += moves\n",
    "            pbar.update(1)\n",
    "            if total_moves > 0:\n",
    "                pbar.set_postfix({'Loss': total_loss / total_moves, 'Accuracy': total_accuracy / total_moves})\n",
    "\n",
    "            del game_batch, loss, accuracy, moves\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    j += 1\n",
    "    if j % 25 == 0:\n",
    "        model_save_path = os.path.join('savedModels', f'cnn_transformer_model_epoch_{j}.pth')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    model_save_path = os.path.join('savedModels', f'cnn_transformer_model_current.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    offset += batch_size\n",
    "    \n",
    "#Close the session and TensorBoard writer\n",
    "#Still have not tried TensorBoard, might not work\n",
    "session.close()\n",
    "model_save_path = os.path.join('savedModels', f'cnn_transformer_model_final.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "#1083 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\lukas\\AppData\\Local\\Temp\\ipykernel_34488\\4067602970.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('savedModels/cnn_transformer_model_epoch_60.pth'))\n",
      "Training Progress:   0%|          | 0/1000 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 10.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 238\u001b[0m\n\u001b[0;32m    234\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_save_path)\n\u001b[0;32m    236\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m--> 238\u001b[0m \u001b[43mtrain_self_play_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 207\u001b[0m, in \u001b[0;36mtrain_self_play_batch\u001b[1;34m(model, optimizer, num_episodes, batch_size, accumulation_steps)\u001b[0m\n\u001b[0;32m    204\u001b[0m episode_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(accumulation_steps):\n\u001b[1;32m--> 207\u001b[0m     log_probs_batch, rewards_batch, checkmate_count \u001b[38;5;241m=\u001b[39m \u001b[43mplay_batch_games\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# Ensure autocast is used only during forward pass\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Enable mixed precision during policy update\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 66\u001b[0m, in \u001b[0;36mplay_batch_games\u001b[1;34m(model, batch_size)\u001b[0m\n\u001b[0;32m     62\u001b[0m states \u001b[38;5;241m=\u001b[39m [board_to_input(boards[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m active_indices]\n\u001b[0;32m     63\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(states)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 66\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     67\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(active_indices):\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 44\u001b[0m, in \u001b[0;36mChessNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(x)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Transformer encoder\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()  \u001b[38;5;66;03m#[batch_size, sequence_length, d_model]\u001b[39;00m\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:416\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    413\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 416\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    419\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:750\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[1;32m--> 750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:765\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 765\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 10.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "###### RL PART ######\n",
    "\n",
    "model = ChessNet().to(device)\n",
    "model.load_state_dict(torch.load('savedModels/cnn_transformer_model_epoch_60.pth'))\n",
    "\n",
    "\n",
    "def board_to_input(board):\n",
    "    board_planes = torch.zeros((8, 8, 12), dtype=torch.float32)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            plane = piece.piece_type - 1\n",
    "            if piece.color == chess.BLACK:\n",
    "                plane += 6\n",
    "            row, col = divmod(square, 8)\n",
    "            board_planes[row, col, plane] = 1\n",
    "    board_input = board_planes.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "    return board_input\n",
    "\n",
    "def move_to_index(move):\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    return from_square * 64 + to_square\n",
    "\n",
    "# Function to select a move given the current board position. Not currently used in the training loop whilst trying to fix some stuff\n",
    "def select_move(model, board):\n",
    "    state = board_to_input(board)\n",
    "    state = state.unsqueeze(0) \n",
    "\n",
    "    \n",
    "    logits = model(state) \n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    legal_indices = [move_to_index(move) for move in legal_moves]\n",
    "\n",
    "    mask = torch.zeros_like(probabilities)\n",
    "    mask[:, legal_indices] = 1  # Mask out illegal moves\n",
    "\n",
    "    legal_probabilities = probabilities * mask  # Apply the mask\n",
    "    legal_probabilities = legal_probabilities / legal_probabilities.sum(dim=1, keepdim=True)  # Re-normalize\n",
    "\n",
    "    m = Categorical(legal_probabilities)\n",
    "    # Sample from moves based on the distribution of probabilities.\n",
    "    # In the early stages of training, the bot will explore many moves that it does not particularly prefer, \n",
    "    # but as the training progresses, the bot will start to get better, the distribution of probabilities of moves will more greatly favor the better moves, \n",
    "    # leading to less exploration.\n",
    "    move_idx = m.sample() \n",
    "\n",
    "    selected_move = legal_moves[move_idx.item()]\n",
    "    return selected_move, m.log_prob(move_idx)\n",
    "\n",
    "# Function to simulate a batch of games of self-play\n",
    "def play_batch_games(model, batch_size):\n",
    "    boards = [chess.Board() for _ in range(batch_size)]\n",
    "    log_probs = [[] for _ in range(batch_size)]\n",
    "    rewards = [[] for _ in range(batch_size)]\n",
    "    previous_material_balances = [material_balance(board) for board in boards]\n",
    "    checkmate_count = 0\n",
    "    while any(not board.is_game_over() for board in boards):\n",
    "        active_indices = [i for i, board in enumerate(boards) if not board.is_game_over()]\n",
    "        states = [board_to_input(boards[i]) for i in active_indices]\n",
    "        states = torch.cat(states).to(device)\n",
    "\n",
    "        \n",
    "        logits = model(states) \n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "        for idx, i in enumerate(active_indices):\n",
    "            legal_moves = list(boards[i].legal_moves)\n",
    "            if len(legal_moves) == 0:\n",
    "                continue  # Skip if no legal moves are available\n",
    "\n",
    "            legal_indices = [move_to_index(move) for move in legal_moves]\n",
    "            mask = torch.zeros_like(probabilities[idx])\n",
    "            mask[legal_indices] = 1\n",
    "            legal_probabilities = probabilities[idx] * mask\n",
    "\n",
    "            if legal_probabilities.sum() == 0:\n",
    "                print(\"All legal probabilities are zero\")\n",
    "                legal_probabilities = mask  # fallback to uniform distribution over legal moves\n",
    "\n",
    "            legal_probabilities = legal_probabilities / legal_probabilities.sum(dim=0, keepdim=True)\n",
    "\n",
    "            m = Categorical(legal_probabilities)\n",
    "            move_idx = m.sample()\n",
    "\n",
    "            log_prob = m.log_prob(move_idx) \n",
    "            log_probs[i].append(log_prob)\n",
    "\n",
    "            move_idx_in_legal_moves = legal_indices.index(move_idx.item()) if move_idx.item() in legal_indices else None\n",
    "\n",
    "            if move_idx_in_legal_moves is None:\n",
    "                continue  # Safeguard against out-of-bound indices\n",
    "\n",
    "            selected_move = legal_moves[move_idx_in_legal_moves]\n",
    "            boards[i].push(selected_move)\n",
    "\n",
    "            if boards[i].is_checkmate():\n",
    "                checkmate_count += 1  # Increment checkmate counter\n",
    "                #Greatly reward the bot for checkmating the opponent\n",
    "                rewards[i].append(5)\n",
    "                break\n",
    "\n",
    "            # If too many moves, break\n",
    "            if len(rewards[i]) > 200:\n",
    "                boards[i].push(chess.Move.null())\n",
    "                # Penalize the bot for taking too many moves\n",
    "                rewards[i][-1] -= 0.5\n",
    "                break\n",
    "\n",
    "            current_material_balance = material_balance(boards[i])\n",
    "            reward = current_material_balance - previous_material_balances[i]\n",
    "\n",
    "            if boards[i].turn == chess.WHITE:  # Bot just played as black\n",
    "                rewards[i].append(-reward)  # Negative reward if bot is black (after white's move)\n",
    "            else:  # Bot just played as white\n",
    "                rewards[i].append(reward)  # Positive reward if bot is white (after black's move)\n",
    "\n",
    "            previous_material_balances[i] = current_material_balance\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        result = boards[i].result()\n",
    "        \n",
    "        for j in range(len(rewards[i])):\n",
    "            rewards[i][j] -= 0.01 # Penalize each move to try make the bot not do many unnecessary moves\n",
    "\n",
    "        if result == '1-0':  # White wins\n",
    "            if len(rewards[i]) % 2 == 1:  \n",
    "                rewards[i][-1] += 1  \n",
    "                rewards[i][-2] -= 1 \n",
    "\n",
    "        elif result == '0-1':  # Black wins\n",
    "            if len(rewards[i]) % 2 == 1:  \n",
    "                rewards[i][-1] += 1  \n",
    "                rewards[i][-2] -= 1  \n",
    "\n",
    "        elif result == '1/2-1/2':  # Draw\n",
    "            rewards[i][-1] += 0.5  \n",
    "            if len(rewards[i]) > 1:\n",
    "                rewards[i][-2] += 0.5  \n",
    "\n",
    "    del states, logits, probabilities\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return log_probs, rewards, checkmate_count\n",
    "\n",
    "\n",
    "\n",
    "def material_balance(board):\n",
    "    white_material = 0\n",
    "    black_material = 0\n",
    "    \n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece is not None:\n",
    "            value = piece_value(piece)\n",
    "            if piece.color == chess.WHITE:\n",
    "                white_material += value\n",
    "            else:\n",
    "                black_material += value\n",
    "\n",
    "    return white_material - black_material  # Positive if white has more material\n",
    "\n",
    "# Function to assign value to pieces TODO: fix right values since these are maybe not good\n",
    "def piece_value(piece):\n",
    "    if piece is None:\n",
    "        return 0\n",
    "    elif piece.piece_type == chess.PAWN:\n",
    "        return 0.1\n",
    "    elif piece.piece_type in [chess.KNIGHT, chess.BISHOP]:\n",
    "        return 0.3\n",
    "    elif piece.piece_type == chess.ROOK:\n",
    "        return 0.5\n",
    "    elif piece.piece_type == chess.QUEEN:\n",
    "        return 0.9\n",
    "    return 0\n",
    "\n",
    "def update_policy_batch(log_probs_batch, rewards_batch, optimizer, gamma=0.99):\n",
    "    policy_loss = 0\n",
    "\n",
    "    for log_probs, rewards in zip(log_probs_batch, rewards_batch):\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, requires_grad=False).to(device) # requires_grad=False since having problems with torch.no_grad during self-play. Trying to fix it\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5)  # Normalize returns\n",
    "\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss += -log_prob * R\n",
    "\n",
    "    return policy_loss\n",
    "\n",
    "def train_self_play_batch(model, optimizer, num_episodes=1000, batch_size=4, accumulation_steps=4):\n",
    "    model.train()\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler for mixed precision\n",
    "    total_checkmates = 0\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "        episode_loss = 0\n",
    "\n",
    "        for _ in range(accumulation_steps):\n",
    "            log_probs_batch, rewards_batch, checkmate_count = play_batch_games(model, batch_size)\n",
    "            \n",
    "            # Ensure autocast is used only during forward pass\n",
    "            with autocast(device_type=\"cuda\"):  # Enable mixed precision during policy update\n",
    "                policy_loss = update_policy_batch(log_probs_batch, rewards_batch, optimizer)\n",
    "\n",
    "                # Accumulate the loss\n",
    "                episode_loss += policy_loss / accumulation_steps\n",
    "\n",
    "        # Check that episode_loss requires gradients\n",
    "       # assert episode_loss.requires_grad, \"episode_loss does not require gradients.\"\n",
    "\n",
    "        # Backward pass with scaled gradients after accumulation\n",
    "        scaler.scale(episode_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_checkmates += checkmate_count  #Count checkmates as metric for progress in early stages\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = sum(sum(rewards) for rewards in rewards_batch) / batch_size\n",
    "            print(f\"Episode {episode} complete - Average Reward: {avg_reward:.2f}, Checkmates: {total_checkmates / (episode + 1) * batch_size:.2f}%\")\n",
    "            total_checkmates = 0\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            model_save_path = os.path.join('savedModels', f'cnn_transformer_model_rl_{episode}.pth')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_self_play_batch(model, optimizer, num_episodes=1000, batch_size=16, accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\AppData\\Local\\Temp\\ipykernel_38924\\3267276009.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('savedModels/cnn_transformer_model_current.pth'))\n",
      "Training Progress:   0%|          | 0/1000 [00:05<?, ?it/s]\n",
      "Exception ignored in: <function tqdm.__del__ at 0x0000019E6CF56480>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\tqdm\\std.py\", line 1267, in close\n",
      "    if self.disable:\n",
      "       ^^^^^^^^^^^^\n",
      "AttributeError: 'tqdm' object has no attribute 'disable'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 230\u001b[0m\n\u001b[0;32m    227\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m \u001b[43mtrain_self_play_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 211\u001b[0m, in \u001b[0;36mtrain_self_play_batch\u001b[1;34m(model, optimizer, num_episodes, batch_size, accumulate_grad_steps)\u001b[0m\n\u001b[0;32m    208\u001b[0m total_checkmates \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_episodes), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     log_probs_batch, rewards_batch, checkmate_count, move_count \u001b[38;5;241m=\u001b[39m \u001b[43mplay_batch_games\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     update_policy_batch(log_probs_batch, rewards_batch, optimizer, move_count)\n\u001b[0;32m    214\u001b[0m     total_checkmates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m checkmate_count  \u001b[38;5;66;03m# Count checkmates as metric for progress in early stages\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 132\u001b[0m, in \u001b[0;36mplay_batch_games\u001b[1;34m(model, batch_size)\u001b[0m\n\u001b[0;32m    128\u001b[0m     legal_probabilities \u001b[38;5;241m=\u001b[39m mask  \u001b[38;5;66;03m# fallback to uniform distribution over legal moves\u001b[39;00m\n\u001b[0;32m    130\u001b[0m legal_probabilities \u001b[38;5;241m=\u001b[39m legal_probabilities \u001b[38;5;241m/\u001b[39m legal_probabilities\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 132\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlegal_probabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m move_idx \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m    135\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mlog_prob(move_idx)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\distributions\\categorical.py:71\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     68\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     67\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 68\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lukas\\Chess_GitHub\\ChessBot\\.venv\\Lib\\site-packages\\torch\\distributions\\constraints.py:441\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(value \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming your ChessNet model is defined elsewhere\n",
    "model = ChessNet().to(device)\n",
    "model.load_state_dict(torch.load('savedModels/cnn_transformer_model_current.pth'))\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Functions to convert board states and moves\n",
    "def board_to_input(board):\n",
    "    board_planes = torch.zeros((8, 8, 12), dtype=torch.float32)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            plane = piece.piece_type - 1\n",
    "            if piece.color == chess.BLACK:\n",
    "                plane += 6\n",
    "            row, col = divmod(square, 8)\n",
    "            board_planes[row, col, plane] = 1\n",
    "    board_input = board_planes.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "    return board_input.to(device)\n",
    "\n",
    "def move_to_index(move):\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    return from_square * 64 + to_square\n",
    "\n",
    "def piece_value(piece):\n",
    "    if piece is None:\n",
    "        return 0\n",
    "    elif piece.piece_type == chess.PAWN:\n",
    "        return 1\n",
    "    elif piece.piece_type in [chess.KNIGHT, chess.BISHOP]:\n",
    "        return 3\n",
    "    elif piece.piece_type == chess.ROOK:\n",
    "        return 5\n",
    "    elif piece.piece_type == chess.QUEEN:\n",
    "        return 9\n",
    "    return 0\n",
    "\n",
    "def material_balance(board):\n",
    "    white_material = 0\n",
    "    black_material = 0\n",
    "    \n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece is not None:\n",
    "            value = piece_value(piece)\n",
    "            if piece.color == chess.WHITE:\n",
    "                white_material += value\n",
    "            else:\n",
    "                black_material += value\n",
    "\n",
    "    return white_material - black_material  # Positive if white has more material\n",
    "\n",
    "def center_control(board):\n",
    "    central_squares = [chess.D4, chess.E4, chess.D5, chess.E5]\n",
    "    control_score = 0\n",
    "\n",
    "    for square in central_squares:\n",
    "        attackers = board.attackers(chess.WHITE, square)\n",
    "        defenders = board.attackers(chess.BLACK, square)\n",
    "\n",
    "        if len(attackers) > len(defenders):\n",
    "            control_score += 0.1\n",
    "        elif len(attackers) < len(defenders):\n",
    "            control_score -= 0.1\n",
    "\n",
    "    return control_score\n",
    "\n",
    "def select_move(model, board):\n",
    "    state = board_to_input(board)\n",
    "    state = state.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(state)\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    legal_indices = [move_to_index(move) for move in legal_moves]\n",
    "\n",
    "    mask = torch.zeros_like(probabilities)\n",
    "    mask[:, legal_indices] = 1  # Mask out illegal moves\n",
    "\n",
    "    legal_probabilities = probabilities * mask  # Apply the mask\n",
    "    legal_probabilities = legal_probabilities / legal_probabilities.sum(dim=1, keepdim=True)  # Re-normalize\n",
    "\n",
    "    m = Categorical(legal_probabilities)\n",
    "    move_idx = m.sample()\n",
    "\n",
    "    selected_move = legal_moves[move_idx.item()]\n",
    "    return selected_move, m.log_prob(move_idx)\n",
    "\n",
    "def dynamic_gamma(move_count):\n",
    "    if move_count < 20:\n",
    "        return 0.95  # Early game\n",
    "    elif move_count < 40:\n",
    "        return 0.98  # Mid game\n",
    "    else:\n",
    "        return 0.99  # End game\n",
    "\n",
    "def play_batch_games(model, batch_size):\n",
    "    boards = [chess.Board() for _ in range(batch_size)]\n",
    "    log_probs = [[] for _ in range(batch_size)]\n",
    "    rewards = [[] for _ in range(batch_size)]\n",
    "    previous_material_balances = [material_balance(board) for board in boards]\n",
    "    checkmate_count = 0\n",
    "    move_count = 0\n",
    "\n",
    "    while any(not board.is_game_over() for board in boards):\n",
    "        active_indices = [i for i, board in enumerate(boards) if not board.is_game_over()]\n",
    "        states = torch.cat([board_to_input(boards[i]) for i in active_indices])\n",
    "\n",
    "        logits = model(states)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "        for idx, i in enumerate(active_indices):\n",
    "            legal_moves = list(boards[i].legal_moves)\n",
    "            if len(legal_moves) == 0:\n",
    "                continue  # Skip if no legal moves are available\n",
    "\n",
    "            legal_indices = [move_to_index(move) for move in legal_moves]\n",
    "            mask = torch.zeros_like(probabilities[idx])\n",
    "            mask[legal_indices] = 1\n",
    "            legal_probabilities = probabilities[idx] * mask\n",
    "\n",
    "            if legal_probabilities.sum() == 0:\n",
    "                legal_probabilities = mask  # fallback to uniform distribution over legal moves\n",
    "\n",
    "            legal_probabilities = legal_probabilities / legal_probabilities.sum(dim=0, keepdim=True)\n",
    "\n",
    "            m = Categorical(legal_probabilities)\n",
    "            move_idx = m.sample()\n",
    "\n",
    "            log_prob = m.log_prob(move_idx)\n",
    "            log_probs[i].append(log_prob)\n",
    "\n",
    "            move_idx_in_legal_moves = legal_indices.index(move_idx.item()) if move_idx.item() in legal_indices else None\n",
    "\n",
    "            if move_idx_in_legal_moves is None:\n",
    "                continue  # Safeguard against out-of-bound indices\n",
    "\n",
    "            selected_move = legal_moves[move_idx_in_legal_moves]\n",
    "            boards[i].push(selected_move)\n",
    "\n",
    "            if boards[i].is_checkmate():\n",
    "                checkmate_count += 1  # Increment checkmate counter\n",
    "                break\n",
    "\n",
    "            current_material_balance = material_balance(boards[i])\n",
    "            current_control_score = center_control(boards[i])\n",
    "            reward = current_material_balance - previous_material_balances[i] + current_control_score\n",
    "\n",
    "            if boards[i].turn == chess.WHITE:  # Bot just played as black\n",
    "                rewards[i].append(-reward)  # Negative reward if bot is black (after white's move)\n",
    "            else:  # Bot just played as white\n",
    "                rewards[i].append(reward)  # Positive reward if bot is white (after black's move)\n",
    "\n",
    "            previous_material_balances[i] = current_material_balance\n",
    "            move_count += 1\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        result = boards[i].result()\n",
    "\n",
    "        for j in range(len(rewards[i])):\n",
    "            rewards[i][j] -= 0.01  # Penalize each move to try make the bot not do many unnecessary moves\n",
    "\n",
    "        if result == '1-0':  # White wins\n",
    "            if len(rewards[i]) % 2 == 1:\n",
    "                rewards[i][-1] += 1\n",
    "                rewards[i][-2] -= 1\n",
    "\n",
    "        elif result == '0-1':  # Black wins\n",
    "            if len(rewards[i]) % 2 == 1:\n",
    "                rewards[i][-1] += 1\n",
    "                rewards[i][-2] -= 1\n",
    "\n",
    "        elif result == '1/2-1/2':  # Draw\n",
    "            rewards[i][-1] += 0.5\n",
    "            if len(rewards[i]) > 1:\n",
    "                rewards[i][-2] += 0.5\n",
    "\n",
    "    return log_probs, rewards, checkmate_count, move_count\n",
    "\n",
    "def update_policy_batch(log_probs_batch, rewards_batch, optimizer, move_count):\n",
    "    optimizer.zero_grad()\n",
    "    gamma = dynamic_gamma(move_count)\n",
    "\n",
    "    for log_probs, rewards in zip(log_probs_batch, rewards_batch):\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5)  # Normalize returns\n",
    "\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            loss = -log_prob * R\n",
    "            scaler.scale(loss).backward(retain_graph=True)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "def train_self_play_batch(model, optimizer, num_episodes=1000, batch_size=8, accumulate_grad_steps=4):\n",
    "    model.train()\n",
    "    total_checkmates = 0\n",
    "\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "        log_probs_batch, rewards_batch, checkmate_count, move_count = play_batch_games(model, batch_size)\n",
    "        update_policy_batch(log_probs_batch, rewards_batch, optimizer, move_count)\n",
    "\n",
    "        total_checkmates += checkmate_count  # Count checkmates as metric for progress in early stages\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = sum(sum(rewards) for rewards in rewards_batch) / batch_size\n",
    "            print(f\"Episode {episode} complete - Average Reward: {avg_reward:.2f}, Checkmates: {total_checkmates/(episode+1)*batch_size:.2f}%\")\n",
    "            total_checkmates = 0\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            model_save_path = os.path.join('savedModels', f'cnn_transformer_model_rl_{episode}.pth')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            \n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Start training\n",
    "train_self_play_batch(model, optimizer, num_episodes=1000, batch_size=8, accumulate_grad_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChessNet()\n",
    "model.load_state_dict(torch.load('savedModels/cnn_transformer_model_current.pth'))\n",
    "model.eval()\n",
    "\n",
    "board = chess.Board()\n",
    "\n",
    "def board_to_input(board):\n",
    "    board_planes = torch.zeros((8, 8, 12), dtype=torch.float32)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            plane = piece.piece_type - 1\n",
    "            if piece.color == chess.BLACK:\n",
    "                plane += 6\n",
    "            row, col = divmod(square, 8)\n",
    "            board_planes[row, col, plane] = 1\n",
    "    board_input = board_planes.unsqueeze(0).permute(0, 3, 1, 2) \n",
    "    return board_input\n",
    "\n",
    "def predict_move(model, board):\n",
    "    board_input = board_to_input(board)\n",
    "    with torch.no_grad():\n",
    "        output = model(board_input)\n",
    "    \n",
    "    move_scores = output.squeeze().sort(descending=True)\n",
    "    move_indices = move_scores.indices.tolist()\n",
    "    \n",
    "    for move_index in move_indices:\n",
    "        from_square = move_index // 64\n",
    "        to_square = move_index % 64\n",
    "        move = chess.Move(from_square, to_square)\n",
    "        \n",
    "        if move in board.legal_moves:\n",
    "            return move\n",
    "    \n",
    "    # If no valid moves are found (which shouldn't happen), return None\n",
    "    return None\n",
    "\n",
    "\n",
    "while not board.is_game_over():\n",
    "    display(board)  \n",
    "    user_move = input(\"Your move: \")\n",
    "\n",
    "    \n",
    "    try:\n",
    "        move = chess.Move.from_uci(user_move)\n",
    "        if move in board.legal_moves:\n",
    "            board.push(move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "    except ValueError:\n",
    "        print(\"Invalid format. Use UCI format (e.g., e2e4).\")\n",
    "        continue\n",
    "\n",
    "    if board.is_game_over():\n",
    "        break\n",
    "\n",
    "    #Get the bots move\n",
    "    bot_move = predict_move(model, board)\n",
    "    if bot_move:\n",
    "        board.push(bot_move)\n",
    "        print(f\"Bot's move: {bot_move}\")\n",
    "    else:\n",
    "        print(\"Bot could not find a valid move.\")\n",
    "        break\n",
    "\n",
    "print(\"Game over!\")\n",
    "print(f\"Result: {board.result()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
